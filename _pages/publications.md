---
layout: page
permalink: /publications/
title: research
description: In reversed chronological order.
nav: true
nav_order: 2
---

<span style="font-size: 16px; font-weight: bold;">
  <a href="https://arxiv.org/abs/2507.00239" target="_blank">Linearly Decoding Refused Knowledge in Aligned Language Models</a>
</span>  
<span style="font-weight: bold; text-decoration: underline;">Aryan Shrivastava</span> and Ari Holtzman<br>
*Under Review*, 2025

<span style="font-size: 16px; font-weight: bold;">
  <a href="https://arxiv.org/abs/2506.11440" target="_blank">AbsenceBench: Language Models Can't Tell What's Missing</a>
</span>  
Harvey Yiyun Fu, <span style="font-weight: bold; text-decoration: underline;">Aryan Shrivastava</span>, Jared Moore, Peter West, Chenhao Tan, Ari Holtzman<br>
*Under Review*, 2025

<span style="font-size: 16px; font-weight: bold;">
  <a href="https://arxiv.org/abs/2504.10359" target="_blank">DICE: A Framework for Dimensional and Contextual Evaluation of Language Models</a>
</span>  
<span style="font-weight: bold; text-decoration: underline;">Aryan Shrivastava</span> and Paula Akemi Aoyagui<br>
*CHI 2025 Human-Centered Evaluation and Auditing of Language Models (HEAL) Workshop*, 2025

<span style="font-size: 16px; font-weight: bold;">
  <a href="https://www.arxiv.org/abs/2502.16051" target="_blank">Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare</a>
</span>  
Max Lamparth, Declan Grabb, Amy Franks, Scott Gershan, Kaitlyn N. Kunstman, Aaron Lulla, Monika Drummond Roots, Manu Sharma, <span style="font-weight: bold; text-decoration: underline;">Aryan Shrivastava</span>, Nina Vasan, Colleen Waickman<br>
*Under Review*, 2025

<span style="font-size: 16px; font-weight: bold;">
  <a href="https://arxiv.org/abs/2410.13204" target="_blank">Measuring Free-Form Decision-Making Inconsistency of Language Models in Military Crisis Simulations</a>
</span>  
<span style="font-weight: bold; text-decoration: underline;">Aryan Shrivastava</span>, Jessica Hullman, Max Lamparth<br>
*NeurIPS 2024 Socially Responsible Language Modelling Research (SoLaR) Workshop; MILA 2024 Harms and Risks of AI in Military Workshop*, 2024
